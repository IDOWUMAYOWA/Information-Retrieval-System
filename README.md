# ğŸ“„ Information Retrieval System (RAG with Gemini + FAISS)

An interactive **PDF-based Question Answering system** built using:

- ğŸ§  Google Gemini (via `langchain-google-genai`)
- ğŸ” FAISS vector database
- ğŸ§© LangChain Conversational Retrieval Chain
- ğŸŒ Streamlit frontend
- ğŸ“š PyPDF2 for document parsing

Upload PDFs, ask questions, and get contextual answers powered by Retrieval-Augmented Generation (RAG).

---

## ğŸš€ Features

- Upload multiple PDF files
- Automatic text extraction
- Intelligent text chunking
- Vector embeddings using Gemini
- FAISS-based similarity search
- Conversational memory
- Clean Streamlit UI
- Context-aware follow-up questions

---

## ğŸ—ï¸ Project Structure

```
Information-Retrieval-System/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ experiments.ipynb
```

---

## âš™ï¸ Tech Stack

- LangChain
- Google Gemini (1.5 Flash)
- FAISS (CPU)
- Streamlit
- PyPDF2
- Python 3.9+

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/Information-Retrieval-System.git
cd Information-Retrieval-System
```

### 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate      # Mac/Linux
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Setup

Create a `.env` file in the root directory:

```
GOOGLE_API_KEY=your_google_api_key_here
```

### Get a Google API key

1. Visit: https://makersuite.google.com/app/apikey  
2. Generate an API key  
3. Paste it into your `.env` file  

If the key is missing, the app raises:

```python
ValueError("GOOGLE_API_KEY not found")
```

---

## â–¶ï¸ Run the Application

```bash
streamlit run app.py
```

Open the browser link shown in the terminal (usually `http://localhost:8501`).

---

## ğŸ§  How It Works

### Step 1: PDF Upload
Users upload one or more PDFs via Streamlit.

### Step 2: Text Extraction
`PyPDF2` extracts raw text from each page.

### Step 3: Text Chunking

```python
RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
```

### Step 4: Embeddings

```python
GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004"
)
```

Each chunk is converted into a dense vector.

### Step 5: Vector Store

```python
FAISS.from_texts(text_chunks, embedding=embeddings)
```

### Step 6: Conversational Retrieval Chain

```python
ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.3
)
```

Uses:
- ConversationBufferMemory
- ConversationalRetrievalChain

Enables:
- Context-aware answers
- Follow-up questions
- Chat memory retention

---

## ğŸ–¥ï¸ User Flow

1. Upload PDFs  
2. Click **Submit**  
3. Wait for processing  
4. Ask a question  
5. Receive context-based answer  
6. Continue the conversation naturally  

---

## ğŸ› ï¸ Customization

### Change Model

```python
model="gemini-1.5-flash"
```

Switch to:

```
gemini-1.5-pro
```

### Adjust Chunk Size

```python
chunk_size=1000
chunk_overlap=200
```

### Adjust Temperature

```python
temperature=0.3
```

- Lower â†’ More factual  
- Higher â†’ More creative  

---

## ğŸ“Œ Use Cases

- Research paper Q&A  
- Company document assistant  
- Academic study tool  
- Legal document analysis  
- Technical documentation chatbot  

---

## âš ï¸ Limitations

- Large PDFs increase processing time  
- FAISS index is in-memory only  
- No authentication system  
- No metadata filtering  
- No OCR support for scanned PDFs  

---

## ğŸ§ª Example Questions

- "Summarize the main findings of the document."
- "Explain section 3 in simple terms."
- "What limitations are mentioned?"
- "What does the author conclude?"

---

## ğŸ Deployment Options

- Streamlit Cloud  
- Render  
- Railway  
- HuggingFace Spaces  
- Docker  

---

## ğŸ§‘â€ğŸ’» Author

IDOWU 
AI / ML Developer  

---

## ğŸ“œ License

This project is licensed under the MIT License.